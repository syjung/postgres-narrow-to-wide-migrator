"""
Wide Table Data Export Web Service
ì„ ë°•ë³„ wide table ë°ì´í„°ë¥¼ Excelë¡œ ì¶”ì¶œí•˜ëŠ” ì›¹ ì„œë¹„ìŠ¤ (ì‹œíŠ¸ë³„ ë¶„ë¦¬)
"""
from flask import Flask, render_template, request, send_file, jsonify
from datetime import datetime, timedelta
from pathlib import Path
import csv
import io
import time
import sys
from typing import Dict, List, Any, Optional
from loguru import logger

from database import db_manager
from channel_router import channel_router
from config import web_export_config

# ë¡œê¹… ì„¤ì •
logger.remove()
logger.add(
    "logs/web_export.log",
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}",
    level="INFO",
    rotation="10 MB"
)
logger.add(
    sys.stdout,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level}</level> | {message}",
    level="INFO"
)

app = Flask(__name__)

# Flask ê¸°ë³¸ ë¡œê±° ë¹„í™œì„±í™”
import logging
log = logging.getLogger('werkzeug')
log.setLevel(logging.ERROR)

# ì„ ë°• ë§¤í•‘ (upsert_migration_data.pyì™€ ë™ì¼)
SHIP_MAPPING = {
    'H2546': 'IMO9976903',
    'H2547': 'IMO9976915',
    'H2548': 'IMO9976927',
    'H2549': 'IMO9976939',
    'H2559': 'IMO9986051',
    'H2560': 'IMO9986087',
}

# ì—­ ë§¤í•‘ (IMO -> H ì½”ë“œ)
IMO_TO_H_CODE = {v: k for k, v in SHIP_MAPPING.items()}


class DataExporter:
    """Wide table ë°ì´í„°ë¥¼ CSVë¡œ ì¶”ì¶œí•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.channel_router = channel_router
    
    def get_ship_data_range(self, ship_code: str) -> Dict[str, Any]:
        """
        ì„ ë°•ì˜ ë°ì´í„° ë²”ìœ„ ì¡°íšŒ (min/max created_time)
        
        Args:
            ship_code: ì„ ë°• ì½”ë“œ (H2546, etc.)
            
        Returns:
            {'min_date': datetime, 'max_date': datetime, 'has_data': bool}
        """
        if ship_code not in SHIP_MAPPING:
            logger.debug(f"Ship code {ship_code} not in SHIP_MAPPING")
            return {'has_data': False}
        
        imo_number = SHIP_MAPPING[ship_code]
        
        # Table 1ì—ì„œ ëŒ€í‘œë¡œ ì¡°íšŒ (ê°€ì¥ ë¹ ë¦„)
        table_name = f"tbl_data_timeseries_{imo_number.lower()}_1"
        logger.debug(f"Checking table: {table_name}")
        
        if not db_manager.check_table_exists(table_name):
            logger.debug(f"Table {table_name} does not exist")
            return {'has_data': False}
        
        try:
            query = f"""
                SELECT 
                    MIN(created_time) as min_date,
                    MAX(created_time) as max_date,
                    COUNT(*) as row_count
                FROM tenant.{table_name}
            """
            
            logger.debug(f"Executing query: {query}")
            result = db_manager.execute_query(query)
            logger.debug(f"Query result: {result}")
            
            if result and len(result) > 0 and result[0]['row_count'] > 0:
                logger.debug(f"Found data for {ship_code}: {result[0]['min_date']} ~ {result[0]['max_date']}, {result[0]['row_count']} rows")
                return {
                    'has_data': True,
                    'min_date': result[0]['min_date'],
                    'max_date': result[0]['max_date'],
                    'row_count': result[0]['row_count']
                }
            else:
                logger.debug(f"No data found for {ship_code} (result: {result})")
                return {'has_data': False}
                
        except Exception as e:
            logger.error(f"Could not get data range for {ship_code}: {e}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            return {'has_data': False}
    
    def export_data(self, ship_code: str, start_date: datetime, end_date: datetime, request_id: str = None) -> Dict[str, Any]:
        """
        ë°ì´í„° ì¶”ì¶œ (3ê°œ í…Œì´ë¸”ì„ created_time ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©)
        
        Args:
            ship_code: ì„ ë°• ì½”ë“œ (H2546, etc.)
            start_date: ì‹œì‘ ì¼ì‹œ
            end_date: ì¢…ë£Œ ì¼ì‹œ
            request_id: ì§„í–‰ìƒí™© ì¶”ì ìš© ID
            
        Returns:
            ì¶”ì¶œ ê²°ê³¼ ì •ë³´
        """
        if ship_code not in SHIP_MAPPING:
            raise ValueError(f"Unknown ship code: {ship_code}")
        
        imo_number = SHIP_MAPPING[ship_code]
        
        logger.info(f"ğŸš€ Starting export for {ship_code} ({imo_number})")
        logger.info(f"   Period: {start_date} ~ {end_date}")
        
        # 3ê°œ í…Œì´ë¸”ì—ì„œ ë°ì´í„° ì¡°íšŒ
        export_info = {
            'ship_code': ship_code,
            'imo_number': imo_number,
            'start_date': start_date,
            'end_date': end_date,
            'tables': {}
        }
        
        # ì§„í–‰ìƒí™© ì—…ë°ì´íŠ¸: ë°ì´í„° ì¡°íšŒ ì‹œì‘
        if request_id:
            export_progress[request_id] = {
                'status': 'processing',
                'message': 'ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ëŠ” ì¤‘...',
                'progress': 20,
                'start_time': export_progress.get(request_id, {}).get('start_time', time.time())
            }
            logger.info(f"ğŸ“Š Progress updated: {request_id} - processing (20%)")
        
        # ê° í…Œì´ë¸”ë³„ë¡œ ë°ì´í„° ì¡°íšŒ (ë³‘ë ¬ ì²˜ë¦¬)
        table_data = {}  # {table_type: (data, columns)}
        
        # ë³‘ë ¬ ì¡°íšŒë¥¼ ìœ„í•œ ThreadPoolExecutor
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        def fetch_table(table_type):
            table_name = f"tbl_data_timeseries_{imo_number.lower()}_{table_type}"
            
            # í…Œì´ë¸” ì¡´ì¬ í™•ì¸
            if not db_manager.check_table_exists(table_name):
                return table_type, None, {
                    'name': table_name,
                    'exists': False,
                    'rows': 0,
                    'columns': 0
                }
            
            # ë°ì´í„° ì¡°íšŒ
            data, columns = self.fetch_table_data(table_name, start_date, end_date)
            
            return table_type, (data, columns), {
                'name': table_name,
                'exists': True,
                'rows': len(data),
                'columns': len(columns),
                'column_list': columns[:10]  # ìƒ˜í”Œ 10ê°œ
            }
        
        # 3ê°œ í…Œì´ë¸” ë³‘ë ¬ ì¡°íšŒ
        with ThreadPoolExecutor(max_workers=3) as executor:
            futures = {executor.submit(fetch_table, t): t for t in ['1', '2', '3']}
            
            completed_tables = 0
            for future in as_completed(futures):
                table_type, data_tuple, table_info = future.result()
                
                if data_tuple:
                    table_data[table_type] = data_tuple
                
                export_info['tables'][table_type] = table_info
                completed_tables += 1
                
                # ì§„í–‰ìƒí™© ì—…ë°ì´íŠ¸: í…Œì´ë¸” ì¡°íšŒ ì™„ë£Œ
                if request_id:
                    progress = 20 + (completed_tables * 20)  # 20% -> 40% -> 60%
                    export_progress[request_id] = {
                        'status': 'processing',
                        'message': f'í…Œì´ë¸” ì¡°íšŒ ì¤‘... ({completed_tables}/3)',
                        'progress': progress,
                        'start_time': export_progress.get(request_id, {}).get('start_time', time.time())
                    }
                    logger.info(f"ğŸ“Š Progress updated: {request_id} - processing ({progress}%)")
        
        # ì§„í–‰ìƒí™© ì—…ë°ì´íŠ¸: ë³‘í•© ì‹œì‘
        if request_id:
            export_progress[request_id] = {
                'status': 'processing',
                'message': 'ë°ì´í„°ë¥¼ ë³‘í•©í•˜ëŠ” ì¤‘...',
                'progress': 70,
                'start_time': export_progress.get(request_id, {}).get('start_time', time.time())
            }
            logger.info(f"ğŸ“Š Progress updated: {request_id} - processing (70%)")
        
        # created_time ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©
        merged_data, all_columns = self.merge_tables_by_timestamp(table_data)
        
        export_info['total_rows'] = len(merged_data)
        export_info['total_columns'] = len(all_columns)
        
        # ì§„í–‰ìƒí™© ì—…ë°ì´íŠ¸: CSV ìƒì„± ì‹œì‘
        if request_id:
            export_progress[request_id] = {
                'status': 'processing',
                'message': 'CSV íŒŒì¼ì„ ìƒì„±í•˜ëŠ” ì¤‘...',
                'progress': 90,
                'start_time': export_progress.get(request_id, {}).get('start_time', time.time())
            }
            logger.info(f"ğŸ“Š Progress updated: {request_id} - processing (90%)")
        
        # CSV íŒŒì¼ ìƒì„±
        csv_content, file_size = self.create_merged_csv(merged_data, all_columns, ship_code, start_date, end_date)
        
        export_info['file_size'] = file_size
        export_info['csv_content'] = csv_content
        
        return export_info
    
    def fetch_table_data(self, table_name: str, start_date: datetime, end_date: datetime, chunk_size: int = 50000) -> tuple:
        """
        í…Œì´ë¸”ì—ì„œ ë°ì´í„° ì¡°íšŒ (COPY ëª…ë ¹ ì‚¬ìš©ìœ¼ë¡œ ìµœì í™”)
        
        Returns:
            (data_rows, column_names)
        """
        logger.info(f"   ğŸ“Š Fetching data from {table_name} using COPY...")
        fetch_start = time.time()
        
        # ì»¬ëŸ¼ ëª©ë¡ ì¡°íšŒ
        col_query = """
            SELECT column_name
            FROM information_schema.columns
            WHERE table_schema = 'tenant'
              AND table_name = %s
            ORDER BY ordinal_position
        """
        
        col_result = db_manager.execute_query(col_query, (table_name,))
        columns = [row['column_name'] for row in col_result] if col_result else []
        
        if not columns:
            logger.warning(f"      âš ï¸ No columns found for {table_name}")
            return [], []
        
        logger.info(f"      Found {len(columns)} columns")
        
        # í…Œì´ë¸” í†µê³„ ì—…ë°ì´íŠ¸ (ì„±ëŠ¥ ìµœì í™”)
        try:
            stats_query = f"ANALYZE tenant.{table_name}"
            db_manager.execute_update(stats_query)
            logger.debug(f"      Updated table statistics for query optimization")
        except Exception as e:
            logger.debug(f"      Could not update statistics: {e}")
        
        # COPY ëª…ë ¹ìœ¼ë¡œ ë°ì´í„° ì¡°íšŒ (í›¨ì”¬ ë¹ ë¦„)
        quoted_columns = [f'"{col}"' if col != 'created_time' else col for col in columns]
        columns_str = ', '.join(quoted_columns)
        
        copy_query = f"""
            COPY (
                SELECT {columns_str}
                FROM tenant.{table_name}
                WHERE created_time >= %s
                  AND created_time < %s
                ORDER BY created_time
            ) TO STDOUT WITH CSV HEADER
        """
        
        logger.info(f"      Executing COPY query for period {start_date.date()} ~ {end_date.date()}...")
        
        try:
            # ë°©ë²• 1: ë‹¨ì¼ COPY + ë°”ì´ë„ˆë¦¬ ì••ì¶• (ìµœê³  ì„±ëŠ¥)
            conn = db_manager.get_connection()
            
            # ë‚ ì§œë¥¼ PostgreSQL í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…
            start_date_str = start_date.strftime('%Y-%m-%d %H:%M:%S')
            end_date_str = end_date.strftime('%Y-%m-%d %H:%M:%S')
            
            logger.info(f"      ğŸ” Executing single COPY with binary compression...")
            query_start = time.time()
            
            # ë°”ì´ë„ˆë¦¬ ì••ì¶• COPY ì¿¼ë¦¬
            copy_query = f"""
                COPY (
                    SELECT {columns_str}
                    FROM tenant.{table_name}
                    WHERE created_time >= '{start_date_str}'
                      AND created_time < '{end_date_str}'
                    ORDER BY created_time
                ) TO STDOUT WITH (FORMAT binary)
            """
            
            # COPY ì‹¤í–‰ì„ ìœ„í•œ ì»¤ì„œ ìƒì„±
            cursor = conn.cursor()
            
            # ë°”ì´ë„ˆë¦¬ ë°ì´í„°ë¥¼ ë°›ê¸° ìœ„í•œ BytesIO
            import io
            copy_buffer = io.BytesIO()
            
            # ë°”ì´ë„ˆë¦¬ COPY ì‹¤í–‰
            cursor.copy_expert(copy_query, copy_buffer)
            
            query_time = time.time() - query_start
            if query_time > 10:  # 10ì´ˆ ì´ìƒ ê±¸ë¦¬ë©´ ê²½ê³ 
                logger.warning(f"      âš ï¸ Slow binary COPY detected: {query_time:.2f}s execution time")
            
            # ë°”ì´ë„ˆë¦¬ ë°ì´í„°ë¥¼ íŒŒì‹±í•˜ì—¬ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            copy_buffer.seek(0)
            binary_data = copy_buffer.getvalue()
            
            logger.info(f"      ğŸ” Parsing binary COPY data ({len(binary_data):,} bytes)...")
            parse_start = time.time()
            
            all_data = self._parse_binary_copy_data(binary_data, columns)
            
            parse_time = time.time() - parse_start
            logger.info(f"      âœ… Binary parsing completed in {parse_time:.2f}s")
            
            cursor.close()
            db_manager.return_connection(conn)
            
            fetch_time = time.time() - fetch_start
            row_count = len(all_data)
            
            if row_count > 50000:  # í° í…Œì´ë¸”ì— ëŒ€í•œ íŠ¹ë³„ ë¡œê·¸
                logger.info(f"      âœ… Large table query completed: {row_count:,} rows in {fetch_time:.2f}s")
            
            logger.info(f"      âœ… Fetched {row_count:,} rows from {table_name} in {fetch_time:.2f}s (BINARY COPY)")
            
            return all_data, columns
            
        except Exception as e:
            logger.warning(f"      âš ï¸ Binary COPY failed, falling back to SELECT: {e}")
            
            # ì—°ê²° ì •ë¦¬
            try:
                if 'conn' in locals():
                    conn.rollback()
                    db_manager.return_connection(conn)
            except Exception as cleanup_error:
                logger.error(f"      âš ï¸ Error during cleanup: {cleanup_error}")
            
            # Fallback to SELECT with parameterized query
            data_query = f"""
                SELECT {columns_str}
                FROM tenant.{table_name}
                WHERE created_time >= %s
                  AND created_time < %s
                ORDER BY created_time
            """
            
            logger.info(f"      Executing SELECT query for period {start_date.date()} ~ {end_date.date()}...")
            
            # Execute with proper parameter binding and performance monitoring
            select_start = time.time()
            data = db_manager.execute_query(data_query, (start_date, end_date))
            select_time = time.time() - select_start
            
            if select_time > 10:  # 10ì´ˆ ì´ìƒ ê±¸ë¦¬ë©´ ê²½ê³ 
                logger.warning(f"      âš ï¸ Slow SELECT query detected: {select_time:.2f}s execution time")
            
            fetch_time = time.time() - fetch_start
            row_count = len(data) if data else 0
            logger.info(f"      âœ… Fetched {row_count:,} rows from {table_name} in {fetch_time:.2f}s (SELECT)")
            
            return data if data else [], columns
    
    def _create_date_chunks(self, start_date: datetime, end_date: datetime, chunk_days: int = 30) -> List[tuple]:
        """
        ë‚ ì§œ ë²”ìœ„ë¥¼ ì²­í¬ë¡œ ë¶„í• 
        
        Args:
            start_date: ì‹œì‘ ë‚ ì§œ
            end_date: ì¢…ë£Œ ë‚ ì§œ
            chunk_days: ì²­í¬ í¬ê¸° (ì¼ ë‹¨ìœ„)
            
        Returns:
            [(chunk_start, chunk_end, chunk_idx), ...]
        """
        chunks = []
        current_start = start_date
        chunk_idx = 1
        
        while current_start < end_date:
            current_end = min(current_start + timedelta(days=chunk_days), end_date)
            chunks.append((current_start, current_end, chunk_idx))
            current_start = current_end
            chunk_idx += 1
        
        return chunks
    
    def _parse_binary_copy_data(self, binary_data: bytes, columns: List[str]) -> List[Dict]:
        """
        PostgreSQL ë°”ì´ë„ˆë¦¬ COPY ë°ì´í„°ë¥¼ íŒŒì‹±
        
        Args:
            binary_data: ë°”ì´ë„ˆë¦¬ COPY ë°ì´í„°
            columns: ì»¬ëŸ¼ ëª©ë¡
            
        Returns:
            ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
        """
        try:
            import struct
            
            if len(binary_data) < 11:  # ìµœì†Œ í—¤ë” í¬ê¸°
                logger.warning("      âš ï¸ Binary data too short")
                return []
            
            # PostgreSQL ë°”ì´ë„ˆë¦¬ COPY í—¤ë” í™•ì¸
            header = b'PGCOPY\n\xff\r\n\x00'
            if binary_data[:11] != header:
                logger.warning("      âš ï¸ Invalid binary COPY format")
                return []
            
            logger.debug("      ğŸ” Parsing PostgreSQL binary COPY format...")
            
            # ë°”ì´ë„ˆë¦¬ ë°ì´í„° íŒŒì‹± ì‹œì‘
            pos = 11  # í—¤ë” ì´í›„ë¶€í„° ì‹œì‘
            
            # í”Œë˜ê·¸ í•„ë“œ ì½ê¸° (4ë°”ì´íŠ¸)
            if pos + 4 > len(binary_data):
                return []
            
            flags = struct.unpack('>I', binary_data[pos:pos+4])[0]
            pos += 4
            
            # í™•ì¥ í—¤ë” ê¸¸ì´ ì½ê¸° (4ë°”ì´íŠ¸)
            if pos + 4 > len(binary_data):
                return []
            
            header_length = struct.unpack('>I', binary_data[pos:pos+4])[0]
            pos += 4
            
            # í™•ì¥ í—¤ë” ê±´ë„ˆë›°ê¸°
            pos += header_length
            
            rows = []
            
            # í–‰ ë°ì´í„° íŒŒì‹±
            while pos < len(binary_data):
                # í–‰ì˜ í•„ë“œ ìˆ˜ ì½ê¸° (2ë°”ì´íŠ¸)
                if pos + 2 > len(binary_data):
                    break
                
                field_count = struct.unpack('>H', binary_data[pos:pos+2])[0]
                pos += 2
                
                if field_count == 0xFFFF:  # EOF ë§ˆì»¤
                    break
                
                row = {}
                
                # ê° í•„ë“œ íŒŒì‹±
                for i in range(field_count):
                    if i >= len(columns):
                        break
                    
                    # í•„ë“œ ê¸¸ì´ ì½ê¸° (4ë°”ì´íŠ¸)
                    if pos + 4 > len(binary_data):
                        break
                    
                    field_length = struct.unpack('>I', binary_data[pos:pos+4])[0]
                    pos += 4
                    
                    if field_length == 0xFFFFFFFF:  # NULL ê°’
                        row[columns[i]] = None
                    else:
                        # í•„ë“œ ë°ì´í„° ì½ê¸°
                        if pos + field_length > len(binary_data):
                            break
                        
                        field_data = binary_data[pos:pos+field_length]
                        pos += field_length
                        
                        # ë°ì´í„° íƒ€ì…ì— ë”°ë¥¸ ë³€í™˜
                        try:
                            # ë¬¸ìì—´ë¡œ ë””ì½”ë”© ì‹œë„
                            value = field_data.decode('utf-8')
                            
                            # ìˆ«ì íƒ€ì… ë³€í™˜ ì‹œë„
                            if '.' in value:
                                try:
                                    value = float(value)
                                except ValueError:
                                    pass
                            else:
                                try:
                                    value = int(value)
                                except ValueError:
                                    pass
                            
                            row[columns[i]] = value
                        except UnicodeDecodeError:
                            # ë°”ì´ë„ˆë¦¬ ë°ì´í„°ì¸ ê²½ìš° ë¬¸ìì—´ë¡œ ìœ ì§€
                            row[columns[i]] = field_data.decode('utf-8', errors='replace')
                
                if len(row) > 0:
                    rows.append(row)
            
            logger.info(f"      âœ… Parsed {len(rows):,} rows from binary data")
            return rows
            
        except Exception as e:
            logger.error(f"      âŒ Error parsing binary COPY data: {e}")
            logger.debug(f"      ğŸ” Binary data length: {len(binary_data):,} bytes")
            return []
    
    def merge_tables_by_timestamp(self, table_data: Dict[str, tuple]) -> tuple:
        """
        3ê°œ í…Œì´ë¸”ì„ created_time ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©
        
        Args:
            table_data: {table_type: (data_rows, columns)}
            
        Returns:
            (merged_data_list, all_columns_sorted)
        """
        logger.info(f"   ğŸ”„ Merging tables by created_time...")
        merge_start = time.time()
        
        # created_time -> merged_row ë§¤í•‘
        merged_dict = {}
        
        # ëª¨ë“  ì»¬ëŸ¼ ìˆ˜ì§‘
        all_columns = set()
        
        for table_type, (data, columns) in table_data.items():
            # created_time ì œì™¸í•œ ì»¬ëŸ¼ë“¤
            data_columns = [col for col in columns if col != 'created_time']
            all_columns.update(data_columns)
            
            logger.info(f"      Merging Table {table_type}: {len(data):,} rows, {len(data_columns)} columns")
            
            # ê° rowë¥¼ created_time ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©
            for row in data:
                created_time = row['created_time']
                
                if created_time not in merged_dict:
                    merged_dict[created_time] = {'created_time': created_time}
                
                # ì´ í…Œì´ë¸”ì˜ ì»¬ëŸ¼ë“¤ì„ ë³‘í•©
                for col in data_columns:
                    merged_dict[created_time][col] = row.get(col)
        
        # created_time ìˆœì„œë¡œ ì •ë ¬
        logger.info(f"      Sorting {len(merged_dict):,} unique timestamps...")
        sorted_timestamps = sorted(merged_dict.keys())
        merged_data = [merged_dict[ts] for ts in sorted_timestamps]
        
        # ì»¬ëŸ¼ ì •ë ¬: created_time + ì•ŒíŒŒë²³ ìˆœì„œ
        logger.info(f"      Sorting {len(all_columns):,} columns...")
        sorted_columns = ['created_time'] + sorted(all_columns)
        
        merge_time = time.time() - merge_start
        logger.info(f"   âœ… Merge completed: {len(merged_data):,} rows, {len(sorted_columns):,} columns in {merge_time:.2f}s")
        
        return merged_data, sorted_columns
    
    def create_merged_csv(self, merged_data: List[Dict], all_columns: List[str], 
                          ship_code: str, start_date: datetime, end_date: datetime) -> tuple:
        """
        ë³‘í•©ëœ ë°ì´í„°ë¡œ CSV íŒŒì¼ ìƒì„±
        
        Returns:
            (csv_content, file_size)
        """
        if not merged_data:
            logger.warning("   âš ï¸ No data to create CSV")
            return None, 0
        
        logger.info(f"   ğŸ“ Creating CSV: {len(merged_data):,} rows Ã— {len(all_columns):,} columns")
        csv_start = time.time()
        
        # CSV ìƒì„±
        output = io.StringIO()
        writer = csv.DictWriter(output, fieldnames=all_columns, restval='')
        writer.writeheader()
        
        for row in merged_data:
            # created_time í¬ë§·íŒ…
            if 'created_time' in row and isinstance(row['created_time'], datetime):
                row['created_time'] = row['created_time'].strftime('%Y-%m-%d %H:%M:%S')
            writer.writerow(row)
        
        csv_content = output.getvalue()
        file_size = len(csv_content.encode('utf-8'))
        
        csv_time = time.time() - csv_start
        logger.info(f"   âœ… CSV created: {file_size / 1024 / 1024:.2f} MB in {csv_time:.2f}s")
        
        return csv_content, file_size


# Flask ë¼ìš°íŠ¸
@app.route('/')
def index():
    """ë©”ì¸ í˜ì´ì§€"""
    logger.info(f"ğŸ“„ Main page accessed from {request.remote_addr}")
    return render_template('index.html', ships=SHIP_MAPPING)


@app.route('/api/ships/data-range')
def get_ships_data_range():
    """ëª¨ë“  ì„ ë°•ì˜ ë°ì´í„° ë²”ìœ„ ì¡°íšŒ"""
    try:
        exporter = DataExporter()
        ships_info = {}
        
        # ì‹¤ì œ í…Œì´ë¸” ëª©ë¡ í™•ì¸ (ë””ë²„ê¹…ìš©)
        try:
            table_check_query = """
                SELECT table_name 
                FROM information_schema.tables 
                WHERE table_schema = 'tenant' 
                AND table_name LIKE 'tbl_data_timeseries_%_1'
                ORDER BY table_name
            """
            existing_tables = db_manager.execute_query(table_check_query)
            if existing_tables:
                logger.debug(f"Existing tables: {[t['table_name'] for t in existing_tables]}")
            else:
                logger.warning("No wide tables found in tenant schema")
        except Exception as e:
            logger.error(f"Failed to check existing tables: {e}")
        
        for ship_code, imo_number in SHIP_MAPPING.items():
            logger.debug(f"Checking data range for {ship_code} ({imo_number})")
            data_range = exporter.get_ship_data_range(ship_code)
            
            if data_range['has_data']:
                ships_info[ship_code] = {
                    'imo_number': imo_number,
                    'min_date': data_range['min_date'].strftime('%Y-%m-%d'),
                    'max_date': data_range['max_date'].strftime('%Y-%m-%d'),
                    'row_count': data_range['row_count'],
                    'has_data': True
                }
            else:
                ships_info[ship_code] = {
                    'imo_number': imo_number,
                    'has_data': False
                }
        
        return jsonify(ships_info)
        
    except Exception as e:
        logger.error(f"Failed to get ships data range: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/check-tables')
def check_tables():
    """í…Œì´ë¸” ìƒì„± ìƒíƒœ í™•ì¸ (ë””ë²„ê¹…ìš©)"""
    try:
        # ëª¨ë“  ì„ ë°•ì˜ í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        table_status = {}
        
        for ship_code, imo_number in SHIP_MAPPING.items():
            tables = {}
            for table_type in ['1', '2', '3']:
                table_name = f"tbl_data_timeseries_{imo_number.lower()}_{table_type}"
                exists = db_manager.check_table_exists(table_name)
                tables[table_name] = exists
                
                if exists:
                    # í–‰ ìˆ˜ í™•ì¸
                    try:
                        count_query = f"SELECT COUNT(*) as cnt FROM tenant.{table_name}"
                        count_result = db_manager.execute_query(count_query)
                        row_count = count_result[0]['cnt'] if count_result else 0
                        tables[f"{table_name}_rows"] = row_count
                        
                        # MIN/MAX ì¿¼ë¦¬ë„ í…ŒìŠ¤íŠ¸
                        if row_count > 0:
                            range_query = f"SELECT MIN(created_time) as min_date, MAX(created_time) as max_date FROM tenant.{table_name}"
                            range_result = db_manager.execute_query(range_query)
                            if range_result:
                                tables[f"{table_name}_min_date"] = str(range_result[0]['min_date'])
                                tables[f"{table_name}_max_date"] = str(range_result[0]['max_date'])
                    except Exception as e:
                        tables[f"{table_name}_rows"] = f"Error: {str(e)}"
            
            table_status[ship_code] = {
                'imo_number': imo_number,
                'tables': tables
            }
        
        return jsonify(table_status)
        
    except Exception as e:
        logger.error(f"Failed to check tables: {e}")
        return jsonify({'error': str(e)}), 500


# ì „ì—­ ì§„í–‰ìƒí™© ì €ì¥
export_progress = {}
active_exports = set()  # ì§„í–‰ ì¤‘ì¸ export ìš”ì²­ ì¶”ì 

@app.route('/export', methods=['POST'])
def export():
    """ë°ì´í„° ì¶”ì¶œ"""
    try:
        # ì…ë ¥ê°’ íŒŒì‹±
        ship_code = request.form.get('ship_code')
        year = int(request.form.get('year'))
        month = int(request.form.get('month'))
        day = int(request.form.get('day'))
        period_type = request.form.get('period_type')  # 'day', 'week', 'month'
        period_value = int(request.form.get('period_value', 1))
        
        # ë‚ ì§œ ê³„ì‚° (ê¸°ì¤€ì¼ì = ì¢…ë£Œì¼, ê¸°ê°„ì„ ê³¼ê±°ë¡œ ê³„ì‚°)
        end_date = datetime(year, month, day, 23, 59, 59)  # ê¸°ì¤€ì¼ ëê¹Œì§€
        
        if period_type == 'day':
            start_date = end_date - timedelta(days=period_value)
        elif period_type == 'week':
            start_date = end_date - timedelta(weeks=period_value)
        elif period_type == 'month':
            # ì›” ê³„ì‚° (ëŒ€ëµì )
            start_date = end_date - timedelta(days=period_value * 30)
        else:
            return jsonify({'error': 'Invalid period type'}), 400
        
        # ì‹œì‘ì¼ì€ 00:00:00ìœ¼ë¡œ
        start_date = start_date.replace(hour=0, minute=0, second=0)
        
        logger.info(f"ğŸ“¥ Export request: {ship_code} ({SHIP_MAPPING[ship_code]}), {start_date.date()} ~ {end_date.date()}")
        
        # ì§„í–‰ìƒí™© ì´ˆê¸°í™”
        request_id = f"{ship_code}_{int(time.time())}"
        
        # ì¤‘ë³µ ìš”ì²­ ë°©ì§€ - ë™ì¼í•œ ship_code + ë‚ ì§œ ë²”ìœ„ ì²´í¬
        request_key = f"{ship_code}_{start_date.date()}_{end_date.date()}"
        if request_key in active_exports:
            logger.warning(f"âš ï¸ Duplicate export request detected: {request_key}")
            return jsonify({
                'error': 'ì´ë¯¸ ë™ì¼í•œ ìš”ì²­ì´ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.',
                'request_key': request_key
            }), 409  # Conflict
        
        active_exports.add(request_key)
        
        export_progress[request_id] = {
            'status': 'starting',
            'message': 'ë°ì´í„° ì¶”ì¶œì„ ì‹œì‘í•©ë‹ˆë‹¤...',
            'progress': 10,
            'start_time': time.time(),
            'ship_code': ship_code,
            'start_date': start_date.isoformat(),
            'end_date': end_date.isoformat(),
            'request_key': request_key
        }
        
        # ë°ì´í„° ì¶”ì¶œ
        exporter = DataExporter()
        start_time = time.time()
        
        result = exporter.export_data(ship_code, start_date, end_date, request_id)
        
        extraction_time = time.time() - start_time
        result['extraction_time'] = f"{extraction_time:.2f}s"
        
        # CSVê°€ ì—†ìœ¼ë©´ ì—ëŸ¬
        if not result['csv_content']:
            export_progress[request_id] = {
                'status': 'error',
                'message': 'ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
                'progress': 100,
                'error': 'No data found'
            }
            return jsonify({
                'error': 'No data found',
                'info': result
            }), 404
        
        # íŒŒì¼ëª… ìƒì„±
        start_str = start_date.strftime('%Y%m%d')
        end_str = end_date.strftime('%Y%m%d')
        filename = f"{ship_code}_{start_str}_to_{end_str}.csv"
        
        result['filename'] = filename
        
        # ì§„í–‰ìƒí™© ì™„ë£Œ
        # request_key ë¯¸ë¦¬ ì €ì¥
        request_key = None
        if request_id in export_progress and 'request_key' in export_progress[request_id]:
            request_key = export_progress[request_id]['request_key']
        
        export_progress[request_id] = {
            'status': 'completed',
            'message': f'ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {filename}',
            'progress': 100,
            'result': result,
            'request_key': request_key,  # ìœ ì§€
            'completed_time': time.time(),  # ì™„ë£Œ ì‹œê°„ ì¶”ê°€
            'download_ready': True  # ë‹¤ìš´ë¡œë“œ ì¤€ë¹„ ì™„ë£Œ í”Œë˜ê·¸
        }
        logger.info(f"ğŸ“Š Progress updated: {request_id} - completed (100%)")
        
        # active_exportsì—ì„œ ì œê±° (ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)
        if request_key:
            active_exports.discard(request_key)
        
        logger.success(f"âœ… Export completed: {filename}, {result['total_rows']:,} rows, {result['file_size']/1024/1024:.2f}MB in {extraction_time:.2f}s")
        
        # CSV íŒŒì¼ë¡œ ì‘ë‹µ (Request IDë¥¼ í—¤ë”ì— í¬í•¨)
        csv_bytes = io.BytesIO(result['csv_content'].encode('utf-8-sig'))  # BOM for Excel
        csv_bytes.seek(0)
        
        response = send_file(
            csv_bytes,
            mimetype='text/csv',
            as_attachment=True,
            download_name=filename
        )
        
        # Request IDë¥¼ í—¤ë”ì— ì¶”ê°€ (ë””ë²„ê¹…ìš©)
        response.headers['X-Request-ID'] = request_id
        
        return response
        
    except Exception as e:
        import traceback
        logger.error(f"âŒ Export failed: {e}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        
        # ì—ëŸ¬ ì§„í–‰ìƒí™© ì—…ë°ì´íŠ¸
        if 'request_id' in locals():
            # request_key ë¯¸ë¦¬ ì €ì¥
            request_key = None
            if request_id in export_progress and 'request_key' in export_progress[request_id]:
                request_key = export_progress[request_id]['request_key']
            
            export_progress[request_id] = {
                'status': 'error',
                'message': f'ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
                'progress': 100,
                'error': str(e),
                'request_key': request_key  # ìœ ì§€
            }
            
            # active_exportsì—ì„œ ì œê±° (ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)
            if request_key:
                active_exports.discard(request_key)
        
        return jsonify({
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500


@app.route('/api/export-progress/<request_id>')
def get_export_progress(request_id):
    """Export ì§„í–‰ìƒí™© ì¡°íšŒ"""
    logger.debug(f"ğŸ” Progress check for request_id: {request_id}")
    logger.debug(f"ğŸ“Š Available progress keys: {list(export_progress.keys())}")
    
    if request_id in export_progress:
        progress = export_progress[request_id].copy()
        logger.debug(f"âœ… Found progress: {progress['status']}")
        
        # ì™„ë£Œëœ ìš”ì²­ì€ 30ë¶„ í›„ ì‚­ì œ (ë‹¤ìš´ë¡œë“œ ì™„ë£Œ í›„ì—ë„ ì¶©ë¶„íˆ ìœ ì§€)
        if progress['status'] in ['completed', 'error']:
            # ì™„ë£Œ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ì‚­ì œ (ë” ì •í™•í•¨)
            completed_time = progress.get('completed_time', progress.get('start_time', 0))
            if time.time() - completed_time > 1800:  # 30ë¶„
                logger.debug(f"ğŸ—‘ï¸ Removing old progress: {request_id}")
                if request_id in export_progress:
                    del export_progress[request_id]
                return jsonify({
                    'status': 'expired',
                    'message': 'ìš”ì²­ì´ ë§Œë£Œë˜ì—ˆìŠµë‹ˆë‹¤.',
                    'progress': 100
                }), 410  # Gone
        
        # ì™„ë£Œëœ ê²½ìš° ì¶”ê°€ ì •ë³´ ì œê³µ
        if progress['status'] == 'completed':
            progress['download_status'] = 'ready'
            progress['can_download'] = True
        
        return jsonify(progress)
    else:
        logger.warning(f"âŒ Progress not found for request_id: {request_id}")
        return jsonify({
            'status': 'not_found',
            'message': 'ìš”ì²­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
            'progress': 0,
            'available_keys': list(export_progress.keys())
        }), 404


@app.route('/preview', methods=['POST'])
def preview():
    """ë¯¸ë¦¬ë³´ê¸° (ë‹¤ìš´ë¡œë“œ ì „ ì •ë³´ í™•ì¸)"""
    try:
        # ì…ë ¥ê°’ íŒŒì‹±
        ship_code = request.form.get('ship_code')
        year = int(request.form.get('year'))
        month = int(request.form.get('month'))
        day = int(request.form.get('day'))
        period_type = request.form.get('period_type')
        period_value = int(request.form.get('period_value', 1))
        
        # ë‚ ì§œ ê³„ì‚° (ê¸°ì¤€ì¼ì = ì¢…ë£Œì¼, ê¸°ê°„ì„ ê³¼ê±°ë¡œ ê³„ì‚°)
        end_date = datetime(year, month, day, 23, 59, 59)  # ê¸°ì¤€ì¼ ëê¹Œì§€
        
        if period_type == 'day':
            start_date = end_date - timedelta(days=period_value)
        elif period_type == 'week':
            start_date = end_date - timedelta(weeks=period_value)
        elif period_type == 'month':
            start_date = end_date - timedelta(days=period_value * 30)
        else:
            return jsonify({'error': 'Invalid period type'}), 400
        
        # ì‹œì‘ì¼ì€ 00:00:00ìœ¼ë¡œ
        start_date = start_date.replace(hour=0, minute=0, second=0)
        
        logger.info(f"ğŸ” Preview request: {ship_code} ({SHIP_MAPPING[ship_code]}), {start_date.date()} ~ {end_date.date()}")
        
        # ë°ì´í„° ì¶”ì¶œ ì •ë³´ë§Œ (CSV ìƒì„± ì•ˆ í•¨)
        exporter = DataExporter()
        start_time = time.time()
        
        result = exporter.export_data(ship_code, start_date, end_date)
        
        extraction_time = time.time() - start_time
        
        # íŒŒì¼ëª… ìƒì„±
        start_str = start_date.strftime('%Y%m%d')
        end_str = end_date.strftime('%Y%m%d')
        filename = f"{ship_code}_{start_str}_to_{end_str}.csv"
        
        # ì‘ë‹µ ë°ì´í„°
        file_size_mb = f"{result['file_size'] / 1024 / 1024:.2f} MB"
        
        response = {
            'filename': filename,
            'ship_code': ship_code,
            'imo_number': result['imo_number'],
            'period': f"{start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}",
            'total_rows': result['total_rows'],
            'total_columns': result['total_columns'],
            'file_size_mb': file_size_mb,
            'extraction_time': f"{extraction_time:.2f}s",
            'tables': result['tables']
        }
        
        logger.info(f"âœ… Preview completed: {result['total_rows']:,} rows, {file_size_mb}, {extraction_time:.2f}s")
        
        return jsonify(response)
        
    except Exception as e:
        import traceback
        logger.error(f"âŒ Preview failed: {e}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        return jsonify({
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500


if __name__ == '__main__':
    logger.info(f"ğŸŒ Starting Wide Table Data Export Web Service")
    logger.info(f"ğŸ“Š Host: {web_export_config.host}")
    logger.info(f"ğŸ“Š Port: {web_export_config.port}")
    logger.info(f"ğŸ“Š Debug: {web_export_config.debug}")
    logger.info(f"ğŸš€ Service starting...")
    app.run(
        debug=web_export_config.debug,
        host=web_export_config.host,
        port=web_export_config.port
    )

