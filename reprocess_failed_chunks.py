"""
ì‹¤íŒ¨í•œ ì²­í¬ ì¬ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸
post_proc.csv íŒŒì¼ì„ ì½ì–´ì„œ ì‹¤íŒ¨í•œ ì²­í¬ë“¤ì„ ì¬ì²˜ë¦¬í•©ë‹ˆë‹¤.
"""
import csv
import sys
import time
import argparse
import os
from datetime import datetime
from typing import List, Tuple, Dict, Any
from loguru import logger
from database import db_manager
from table_generator import table_generator
from chunked_migration_strategy import chunked_migration_strategy
from thread_logger import get_ship_thread_logger
from config import migration_config


def setup_reprocess_logger():
    """ì¬ì²˜ë¦¬ ì „ìš© ë¡œê±° ì„¤ì •"""
    # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±° (ì„ íƒì )
    # logger.remove()
    
    # logs ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
    os.makedirs("logs", exist_ok=True)
    
    # íƒ€ì„ìŠ¤íƒ¬í”„ê°€ í¬í•¨ëœ ë¡œê·¸ íŒŒì¼ëª…
    log_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ì¬ì²˜ë¦¬ ì „ìš© ë¡œê·¸ íŒŒì¼ ì¶”ê°€
    logger.add(
        f"logs/reprocess_{log_timestamp}.log",
        format="{time:YYYY-MM-DD HH:mm:ss.SSS} | {level: <8} | {name}:{function}:{line} - {message}",
        level="INFO",
        rotation="100 MB",
        retention="30 days",
        compression="zip"
    )
    
    # ìµœì‹  ë¡œê·¸ íŒŒì¼ ë§í¬ (ì‹¬ë³¼ë¦­ ë§í¬)
    latest_log = "logs/reprocess_latest.log"
    try:
        if os.path.exists(latest_log):
            os.remove(latest_log)
        os.symlink(f"reprocess_{log_timestamp}.log", latest_log)
    except Exception:
        pass  # Windowsì—ì„œëŠ” ì‹¤íŒ¨í•  ìˆ˜ ìˆìŒ
    
    logger.info(f"ğŸ“ Reprocess log file: logs/reprocess_{log_timestamp}.log")
    return f"logs/reprocess_{log_timestamp}.log"


class FailedChunkReprocessor:
    """ì‹¤íŒ¨í•œ ì²­í¬ ì¬ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, csv_file: str = "post_proc.csv", dry_run: bool = False):
        self.csv_file = csv_file
        self.chunked_strategy = chunked_migration_strategy
        self.dry_run = dry_run
        
        # í†µê³„ ì •ë³´
        self.total_chunks = 0
        self.successful_chunks = 0
        self.failed_chunks = 0
        self.skipped_chunks = 0
        self.total_records = 0
        
        # ì‹¤íŒ¨í•œ ì²­í¬ ì¶”ì 
        self.failed_chunk_details: List[Dict[str, Any]] = []
    
    def load_failed_chunks(self) -> List[Tuple[str, datetime, datetime]]:
        """
        CSV íŒŒì¼ì—ì„œ ì‹¤íŒ¨í•œ ì²­í¬ ì •ë³´ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
        
        Returns:
            List of (ship_id, start_time, end_time) tuples
        """
        logger.info(f"ğŸ“‚ Loading failed chunks from {self.csv_file}")
        
        chunks = []
        try:
            with open(self.csv_file, 'r', encoding='utf-8') as f:
                reader = csv.reader(f)
                for line_num, row in enumerate(reader, start=1):
                    if not row or len(row) != 3:
                        logger.warning(f"âš ï¸ Skipping invalid line {line_num}: {row}")
                        continue
                    
                    ship_id = row[0].strip()
                    start_time_str = row[1].strip()
                    end_time_str = row[2].strip()
                    
                    try:
                        # Parse datetime strings
                        start_time = datetime.fromisoformat(start_time_str)
                        end_time = datetime.fromisoformat(end_time_str)
                        
                        chunks.append((ship_id, start_time, end_time))
                    except ValueError as e:
                        logger.error(f"âŒ Failed to parse datetime on line {line_num}: {e}")
                        continue
            
            logger.info(f"âœ… Loaded {len(chunks)} failed chunks")
            return chunks
            
        except FileNotFoundError:
            logger.error(f"âŒ File not found: {self.csv_file}")
            return []
        except Exception as e:
            logger.error(f"âŒ Error loading CSV file: {e}")
            return []
    
    def reprocess_chunk(self, ship_id: str, start_time: datetime, end_time: datetime) -> Dict[str, Any]:
        """
        ë‹¨ì¼ ì²­í¬ë¥¼ ì¬ì²˜ë¦¬í•©ë‹ˆë‹¤.
        
        Args:
            ship_id: ì„ ë°• ID
            start_time: ì²­í¬ ì‹œì‘ ì‹œê°„
            end_time: ì²­í¬ ì¢…ë£Œ ì‹œê°„
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        thread_logger = get_ship_thread_logger(ship_id)
        
        # Dry-run mode
        if self.dry_run:
            thread_logger.info(f"ğŸ” [DRY-RUN] Would reprocess chunk: {ship_id} [{start_time} to {end_time}]")
            table_name = f"tbl_data_timeseries_{ship_id.lower()}"
            thread_logger.info(f"ğŸ” [DRY-RUN] Target table: {table_name}")
            return {
                'status': 'completed',
                'records_processed': 0,
                'message': 'Dry-run mode - no actual processing'
            }
        
        thread_logger.info(f"ğŸ”„ Reprocessing chunk: {ship_id} [{start_time} to {end_time}]")
        
        try:
            # Get target table name
            table_name = f"tbl_data_timeseries_{ship_id.lower()}"
            
            # Ensure table exists
            thread_logger.info(f"ğŸ” Checking if table exists: {table_name}")
            if not table_generator.ensure_ship_table_exists(ship_id):
                thread_logger.error(f"âŒ Failed to ensure table exists: {table_name}")
                return {
                    'status': 'failed',
                    'error': 'Failed to ensure table exists'
                }
            
            # Migrate chunk using chunked strategy
            result = self.chunked_strategy.migrate_chunk(
                ship_id=ship_id,
                start_time=start_time,
                end_time=end_time,
                table_name=table_name,
                thread_logger=thread_logger
            )
            
            return result
            
        except Exception as e:
            thread_logger.error(f"âŒ Failed to reprocess chunk: {e}")
            return {
                'status': 'failed',
                'error': str(e)
            }
    
    def reprocess_all(self, delay_seconds: float = 0.5) -> Dict[str, Any]:
        """
        ëª¨ë“  ì‹¤íŒ¨í•œ ì²­í¬ë¥¼ ì¬ì²˜ë¦¬í•©ë‹ˆë‹¤.
        
        Args:
            delay_seconds: ê° ì²­í¬ ì²˜ë¦¬ ì‚¬ì´ì˜ ì§€ì—° ì‹œê°„ (ì´ˆ)
            
        Returns:
            ì „ì²´ ì¬ì²˜ë¦¬ ê²°ê³¼
        """
        logger.info("=" * 80)
        if self.dry_run:
            logger.info("ğŸ” Starting failed chunk reprocessing (DRY-RUN MODE)")
            logger.warning("âš ï¸  This is a dry-run - no actual processing will be done")
        else:
            logger.info("ğŸš€ Starting failed chunk reprocessing")
        logger.info("=" * 80)
        
        # Load failed chunks
        chunks = self.load_failed_chunks()
        
        if not chunks:
            logger.warning("âš ï¸ No chunks to reprocess")
            return {
                'status': 'completed',
                'total_chunks': 0,
                'successful_chunks': 0,
                'failed_chunks': 0,
                'skipped_chunks': 0,
                'total_records': 0
            }
        
        self.total_chunks = len(chunks)
        logger.info(f"ğŸ“Š Total chunks to reprocess: {self.total_chunks}")
        
        # Process each chunk
        start_time = time.time()
        
        for idx, (ship_id, chunk_start, chunk_end) in enumerate(chunks, start=1):
            logger.info(f"\n{'=' * 80}")
            logger.info(f"ğŸ“¦ Processing chunk {idx}/{self.total_chunks}")
            logger.info(f"   Ship: {ship_id}")
            logger.info(f"   Range: {chunk_start} to {chunk_end}")
            logger.info(f"{'=' * 80}")
            
            # Reprocess chunk
            result = self.reprocess_chunk(ship_id, chunk_start, chunk_end)
            
            # Update statistics
            if result.get('status') == 'completed':
                self.successful_chunks += 1
                records = result.get('records_processed', 0)
                self.total_records += records
                logger.success(f"âœ… Chunk {idx}/{self.total_chunks} completed: {records} records")
            elif result.get('status') == 'skipped':
                self.skipped_chunks += 1
                logger.info(f"â­ï¸ Chunk {idx}/{self.total_chunks} skipped: {result.get('message', 'No data')}")
            else:
                self.failed_chunks += 1
                error = result.get('error', 'Unknown error')
                logger.error(f"âŒ Chunk {idx}/{self.total_chunks} failed: {error}")
                
                # Track failed chunk
                self.failed_chunk_details.append({
                    'ship_id': ship_id,
                    'start_time': chunk_start.isoformat(),
                    'end_time': chunk_end.isoformat(),
                    'error': error
                })
            
            # Progress update
            progress = (idx / self.total_chunks) * 100
            logger.info(f"ğŸ“Š Progress: {progress:.1f}% ({idx}/{self.total_chunks})")
            logger.info(f"ğŸ“Š Stats: âœ… {self.successful_chunks} | âŒ {self.failed_chunks} | â­ï¸ {self.skipped_chunks}")
            
            # Delay between chunks to avoid overwhelming the database
            if delay_seconds > 0 and idx < self.total_chunks:
                time.sleep(delay_seconds)
        
        # Final statistics
        elapsed_time = time.time() - start_time
        
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ Reprocessing completed")
        logger.info("=" * 80)
        logger.info(f"ğŸ“Š Total chunks: {self.total_chunks}")
        logger.info(f"âœ… Successful: {self.successful_chunks}")
        logger.info(f"âŒ Failed: {self.failed_chunks}")
        logger.info(f"â­ï¸ Skipped: {self.skipped_chunks}")
        logger.info(f"ğŸ“ˆ Total records processed: {self.total_records:,}")
        logger.info(f"â±ï¸ Total time: {elapsed_time:.2f} seconds")
        
        if self.total_chunks > 0:
            logger.info(f"ğŸ“Š Success rate: {(self.successful_chunks/self.total_chunks)*100:.1f}%")
            logger.info(f"âš¡ Average time per chunk: {elapsed_time/self.total_chunks:.2f} seconds")
        
        # Save failed chunks to a new file if any
        if self.failed_chunk_details:
            failed_file = "failed_chunks_after_reprocess.csv"
            logger.warning(f"âš ï¸ Saving {len(self.failed_chunk_details)} failed chunks to {failed_file}")
            
            try:
                with open(failed_file, 'w', encoding='utf-8') as f:
                    writer = csv.writer(f)
                    for detail in self.failed_chunk_details:
                        writer.writerow([
                            detail['ship_id'],
                            detail['start_time'],
                            detail['end_time']
                        ])
                logger.info(f"ğŸ’¾ Failed chunks saved to {failed_file}")
            except Exception as e:
                logger.error(f"âŒ Failed to save failed chunks: {e}")
        
        return {
            'status': 'completed',
            'total_chunks': self.total_chunks,
            'successful_chunks': self.successful_chunks,
            'failed_chunks': self.failed_chunks,
            'skipped_chunks': self.skipped_chunks,
            'total_records': self.total_records,
            'elapsed_time': elapsed_time,
            'failed_details': self.failed_chunk_details
        }


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # Parse command line arguments
    parser = argparse.ArgumentParser(
        description='ì‹¤íŒ¨í•œ ì²­í¬ ì¬ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì œ:
  # ê¸°ë³¸ ì‹¤í–‰ (post_proc.csv ì‚¬ìš©)
  python3 reprocess_failed_chunks.py
  
  # ë‹¤ë¥¸ CSV íŒŒì¼ ì‚¬ìš©
  python3 reprocess_failed_chunks.py -f my_failed_chunks.csv
  
  # Dry-run ëª¨ë“œ (ì‹¤ì œ ì²˜ë¦¬ ì—†ì´ í™•ì¸ë§Œ)
  python3 reprocess_failed_chunks.py --dry-run
  
  # ì§€ì—° ì‹œê°„ ì¡°ì •
  python3 reprocess_failed_chunks.py -d 1.0
  
  # ì„ ë°•ë³„ í•„í„°ë§ (íŠ¹ì • ì„ ë°•ë§Œ ì²˜ë¦¬)
  python3 reprocess_failed_chunks.py --ship IMO9986063
        """
    )
    
    parser.add_argument(
        '-f', '--file',
        default='post_proc.csv',
        help='ì‹¤íŒ¨í•œ ì²­í¬ CSV íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: post_proc.csv)'
    )
    
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='Dry-run ëª¨ë“œ (ì‹¤ì œ ì²˜ë¦¬ ì—†ì´ í™•ì¸ë§Œ)'
    )
    
    parser.add_argument(
        '-d', '--delay',
        type=float,
        default=0.5,
        help='ì²­í¬ ì²˜ë¦¬ ì‚¬ì´ì˜ ì§€ì—° ì‹œê°„ (ì´ˆ, ê¸°ë³¸ê°’: 0.5)'
    )
    
    parser.add_argument(
        '--ship',
        type=str,
        help='íŠ¹ì • ì„ ë°•ë§Œ ì²˜ë¦¬ (ì˜ˆ: IMO9986063)'
    )
    
    args = parser.parse_args()
    
    # ì¬ì²˜ë¦¬ ì „ìš© ë¡œê±° ì„¤ì •
    log_file = setup_reprocess_logger()
    
    logger.info("=" * 80)
    logger.info("ğŸ“‹ Failed Chunk Reprocessor")
    logger.info("=" * 80)
    logger.info(f"ğŸ“‚ Input file: {args.file}")
    logger.info(f"ğŸ“ Log file: {log_file}")
    logger.info(f"â±ï¸  Delay: {args.delay} seconds")
    
    if args.dry_run:
        logger.warning("ğŸ” Running in DRY-RUN mode - no actual processing")
    
    if args.ship:
        logger.info(f"ğŸš¢ Filtering by ship: {args.ship}")
    
    # Create reprocessor
    reprocessor = FailedChunkReprocessor(csv_file=args.file, dry_run=args.dry_run)
    
    # Filter by ship if specified
    if args.ship:
        original_load = reprocessor.load_failed_chunks
        def filtered_load():
            chunks = original_load()
            filtered = [(s, st, et) for s, st, et in chunks if s == args.ship]
            logger.info(f"ğŸ“Š Filtered to {len(filtered)} chunks for {args.ship}")
            return filtered
        reprocessor.load_failed_chunks = filtered_load
    
    # Reprocess all failed chunks
    result = reprocessor.reprocess_all(delay_seconds=args.delay)
    
    # Exit with appropriate status code
    if args.dry_run:
        logger.success("âœ… Dry-run completed successfully!")
        sys.exit(0)
    elif result['failed_chunks'] == 0:
        logger.success("ğŸ‰ All chunks processed successfully!")
        sys.exit(0)
    else:
        logger.warning(f"âš ï¸ {result['failed_chunks']} chunks still failed")
        sys.exit(1)


if __name__ == "__main__":
    main()

