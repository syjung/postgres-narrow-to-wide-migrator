"""
CSV Migration Data Upserter
migration_data í´ë”ì˜ CSV íŒŒì¼ë“¤ì„ ì½ì–´ì„œ 3ê°œì˜ wide í…Œì´ë¸”ì— upsert
"""
import os
import csv
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from pathlib import Path
from loguru import logger
import sys

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ import
from database import db_manager
from channel_router import channel_router
from config import migration_config

# ì„ ë°• ë²ˆí˜¸ ë§¤í•‘
SHIP_MAPPING = {
    'H2546': 'IMO9976903',
    'H2547': 'IMO9976915',
    'H2548': 'IMO9976927',
    'H2549': 'IMO9976939',
    'H2559': 'IMO9986051',
    'H2560': 'IMO9986087',
}

# ë¡œê¹… ì„¤ì •
logger.remove()
logger.add(
    "logs/csv_upsert.log",
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}",
    level="INFO",
    rotation="100 MB"
)
logger.add(
    sys.stdout,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level}</level> | {message}",
    level="INFO"
)


class CSVMigrationUpserter:
    """CSV íŒŒì¼ì„ ì½ì–´ì„œ wide í…Œì´ë¸”ì— upsertí•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, base_dir: str = "migration_data", dry_run: bool = False):
        self.base_dir = Path(base_dir)
        self.channel_router = channel_router
        self.dry_run = dry_run
        self.stats = {
            'total_files': 0,
            'processed_files': 0,
            'failed_files': 0,
            'csv_rows_read': 0,  # CSVì—ì„œ ì½ì€ ì‹¤ì œ row ìˆ˜
            'table_1_rows': 0,   # Table 1ì— upsertëœ row ìˆ˜
            'table_2_rows': 0,   # Table 2ì— upsertëœ row ìˆ˜
            'table_3_rows': 0,   # Table 3ì— upsertëœ row ìˆ˜
        }
        
        # í…Œì´ë¸” ì»¬ëŸ¼ ìˆ˜ ìºì‹œ (í…Œì´ë¸”ëª… -> ì»¬ëŸ¼ ìˆ˜)
        self.table_column_count_cache: Dict[str, int] = {}
        
        if dry_run:
            logger.warning("ğŸ” DRY-RUN MODE: No data will be inserted into DB")
    
    def process_all_ships(self):
        """ëª¨ë“  ì„ ë°•ì˜ CSV íŒŒì¼ ì²˜ë¦¬"""
        logger.info("ğŸš€ Starting CSV migration data upsert")
        logger.info(f"ğŸ“‚ Base directory: {self.base_dir.absolute()}")
        
        if not self.base_dir.exists():
            logger.error(f"âŒ Directory not found: {self.base_dir}")
            return
        
        # ê° ì„ ë°• í´ë” ì²˜ë¦¬
        for ship_folder in sorted(self.base_dir.iterdir()):
            if not ship_folder.is_dir():
                continue
            
            ship_code = ship_folder.name
            if ship_code not in SHIP_MAPPING:
                logger.warning(f"âš ï¸ Unknown ship code: {ship_code}, skipping...")
                continue
            
            imo_number = SHIP_MAPPING[ship_code]
            logger.info(f"\n{'='*80}")
            logger.info(f"ğŸš¢ Processing ship: {ship_code} â†’ {imo_number}")
            logger.info(f"{'='*80}")
            
            self.process_ship_folder(ship_folder, imo_number)
        
        # ìµœì¢… í†µê³„
        self.print_summary()
    
    def process_ship_folder(self, ship_folder: Path, imo_number: str):
        """íŠ¹ì • ì„ ë°• í´ë”ì˜ ëª¨ë“  CSV íŒŒì¼ ì²˜ë¦¬"""
        csv_files = sorted(ship_folder.glob("*.csv"))
        
        if not csv_files:
            logger.warning(f"   âš ï¸ No CSV files found in {ship_folder}")
            return
        
        logger.info(f"   ğŸ“Š Found {len(csv_files)} CSV files")
        
        # í…Œì´ë¸” ì¡´ì¬ í™•ì¸ (Dry-runì´ ì•„ë‹ ë•Œë§Œ)
        if not self.dry_run:
            logger.info(f"   ğŸ” Checking if tables exist for {imo_number}...")
            for table_type in ['1', '2', '3']:
                table_name = f"tbl_data_timeseries_{imo_number.lower()}_{table_type}"
                if not db_manager.check_table_exists(table_name):
                    logger.error(f"   âŒ Table does not exist: {table_name}")
                    logger.error(f"   ğŸ’¡ Run Realtime or Batch first to create tables, or use multi_table_generator")
                    raise RuntimeError(f"Table {table_name} does not exist")
            logger.info(f"   âœ… All 3 tables exist")
            
            # í…Œì´ë¸” ì»¬ëŸ¼ ê°œìˆ˜ í™•ì¸ ë° ê²½ê³ 
            self.check_table_columns(imo_number)
        
        for csv_file in csv_files:
            try:
                self.process_csv_file(csv_file, imo_number)
                self.stats['processed_files'] += 1
            except Exception as e:
                logger.error(f"   âŒ Failed to process {csv_file.name}: {e}")
                self.stats['failed_files'] += 1
    
    def process_csv_file(self, csv_file: Path, imo_number: str):
        """ë‹¨ì¼ CSV íŒŒì¼ ì²˜ë¦¬"""
        logger.info(f"\n   ğŸ“„ Processing: {csv_file.name}")
        self.stats['total_files'] += 1
        
        # CSV ì½ê¸°
        with open(csv_file, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            
            # í—¤ë”ì—ì„œ ì±„ë„ ëª©ë¡ ì¶”ì¶œ
            fieldnames = reader.fieldnames
            if not fieldnames or 'timestamp' not in fieldnames:
                raise ValueError(f"Invalid CSV format: missing 'timestamp' column")
            
            # ì›ë³¸ ì±„ë„ ID (CSV í—¤ë” ê·¸ëŒ€ë¡œ)
            channel_ids_original = [col for col in fieldnames if col != 'timestamp']
            logger.info(f"      ğŸ“Š Columns: {len(channel_ids_original)} channels")
            
            # ì±„ë„ì„ í…Œì´ë¸”ë³„ë¡œ ë¶„ë¥˜ (normalizeëœ ID ì‚¬ìš©)
            channels_by_table, channel_mapping = self.classify_channels(channel_ids_original)
            
            # ë§¤ì¹­ëœ ì±„ë„ ì´ ìˆ˜ í™•ì¸
            total_matched = sum(len(chs) for chs in channels_by_table.values())
            if total_matched == 0:
                logger.error(f"      âŒ No channels matched! All {len(channel_ids_original)} channels are unknown.")
                logger.error(f"         Sample unmapped channels: {channel_ids_original[:5]}")
                raise ValueError(f"No channels matched for {csv_file.name}")
            
            if total_matched < len(channel_ids_original):
                unmapped_count = len(channel_ids_original) - total_matched
                logger.warning(f"      âš ï¸ {unmapped_count}/{len(channel_ids_original)} channels not mapped (will be skipped)")
            
            # í…Œì´ë¸”ë³„ í†µê³„ ë° Coverage í™•ì¸
            for table_type, channels in channels_by_table.items():
                if channels:
                    # í…Œì´ë¸”ì˜ ì „ì²´ ì»¬ëŸ¼ ìˆ˜ ì¡°íšŒ
                    table_name = f"tbl_data_timeseries_{imo_number.lower()}_{table_type}"
                    table_col_count = self.get_table_column_count(table_name)
                    
                    csv_col_count = len(channels)
                    coverage = (csv_col_count / table_col_count * 100) if table_col_count > 0 else 0
                    
                    logger.info(f"      - Table {table_type}: {csv_col_count}/{table_col_count} channels ({coverage:.1f}% coverage)")
                    
                    # ì •ë³´: Coverageê°€ ë‚®ìœ¼ë©´ (ê²½ê³  ì•„ë‹˜, ì •ë³´)
                    if coverage < 50 and table_col_count > 0 and not self.dry_run:
                        unmapped_count = table_col_count - csv_col_count
                        logger.info(f"         ğŸ“Š Partial update: {unmapped_count} columns not in CSV (will be NULL for new rows, unchanged for existing rows)")
                else:
                    logger.debug(f"      - Table {table_type}: 0 channels")
            
            # ë°ì´í„° ì²˜ë¦¬
            rows_processed = 0
            batch_size = 1000
            batch_data = {
                '1': [],
                '2': [],
                '3': []
            }
            
            for row in reader:
                timestamp_str = row['timestamp']
                
                # timestamp íŒŒì‹±
                try:
                    timestamp = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S')
                except ValueError:
                    logger.warning(f"      âš ï¸ Invalid timestamp: {timestamp_str}, skipping row")
                    continue
                
                # í…Œì´ë¸”ë³„ë¡œ ë°ì´í„° ì¤€ë¹„
                for table_type, table_channels in channels_by_table.items():
                    # ì´ í…Œì´ë¸”ì— ë§¤ì¹­ë˜ëŠ” ì±„ë„ì´ ì—†ìœ¼ë©´ skip
                    if not table_channels:
                        continue
                    
                    row_data = {'created_time': timestamp}
                    has_valid_data = False
                    
                    for normalized_channel_id in table_channels:
                        # CSVì˜ ì›ë³¸ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ì¡°íšŒ (ë§¤í•‘ ì‚¬ìš©!)
                        original_channel_id = channel_mapping[normalized_channel_id]
                        value_str = row.get(original_channel_id, '')
                        
                        if value_str and value_str.strip():
                            try:
                                value = float(value_str)
                                # DBì—ëŠ” normalized IDë¥¼ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ì‚¬ìš©
                                row_data[normalized_channel_id] = value
                                has_valid_data = True  # ìœ íš¨í•œ ë°ì´í„°ê°€ í•˜ë‚˜ë¼ë„ ìˆìŒ
                            except ValueError:
                                # ë³€í™˜ ì‹¤íŒ¨ ì‹œ None
                                logger.debug(f"         âš ï¸ Failed to convert '{value_str}' to float for {normalized_channel_id}")
                                row_data[normalized_channel_id] = None
                        else:
                            row_data[normalized_channel_id] = None
                    
                    # ìœ íš¨í•œ ë°ì´í„°ê°€ í•˜ë‚˜ë¼ë„ ìˆì„ ë•Œë§Œ ì¶”ê°€
                    # (created_timeë§Œ ìˆëŠ” ë¹ˆ row ë°©ì§€)
                    if has_valid_data:
                        batch_data[table_type].append(row_data)
                
                rows_processed += 1
                
                # ë°°ì¹˜ ì²˜ë¦¬
                if rows_processed % batch_size == 0:
                    self.upsert_batch_data(imo_number, batch_data, channels_by_table)
                    batch_data = {'1': [], '2': [], '3': []}
                    logger.info(f"      â³ Processed {rows_processed} rows...")
            
            # ë‚¨ì€ ë°ì´í„° ì²˜ë¦¬
            if any(batch_data.values()):
                self.upsert_batch_data(imo_number, batch_data, channels_by_table)
            
            self.stats['csv_rows_read'] += rows_processed
            logger.success(f"      âœ… Completed: {rows_processed} CSV rows processed")
    
    def classify_channels(self, channel_ids: List[str]) -> Tuple[Dict[str, List[str]], Dict[str, str]]:
        """
        ì±„ë„ì„ í…Œì´ë¸”ë³„ë¡œ ë¶„ë¥˜
        
        Returns:
            (channels_by_table, channel_mapping)
            - channels_by_table: í…Œì´ë¸”ë³„ normalized ì±„ë„ ë¦¬ìŠ¤íŠ¸
            - channel_mapping: normalized_id -> original_id ë§¤í•‘
        """
        channels_by_table = {
            '1': [],  # auxiliary_systems
            '2': [],  # engine_generator
            '3': []   # navigation_ship
        }
        
        # Normalized -> Original ë§¤í•‘
        channel_mapping = {}
        unmapped_channels = []
        
        for original_id in channel_ids:
            # ê³µë°± ì œê±° ë° normalize (ì¤‘ìš”!)
            normalized_id = original_id.strip()
            
            table_type = self.channel_router.get_table_type(normalized_id)
            if table_type:
                channels_by_table[table_type].append(normalized_id)
                # ë§¤í•‘ ì €ì¥: normalized -> original (CSV ì¡°íšŒìš©)
                channel_mapping[normalized_id] = original_id
            else:
                unmapped_channels.append(original_id)
        
        # Unmapped channels ìƒì„¸ ë¡œê·¸
        if unmapped_channels:
            logger.warning(f"         âš ï¸ {len(unmapped_channels)} unmapped channels (will be skipped):")
            # ì²˜ìŒ 10ê°œë§Œ ìƒ˜í”Œë¡œ í‘œì‹œ
            for ch in unmapped_channels[:10]:
                logger.warning(f"            - '{ch}'")
            if len(unmapped_channels) > 10:
                logger.warning(f"            ... and {len(unmapped_channels) - 10} more")
        
        return channels_by_table, channel_mapping
    
    def get_table_column_count(self, table_name: str) -> int:
        """
        í…Œì´ë¸”ì˜ ë°ì´í„° ì»¬ëŸ¼ ìˆ˜ ì¡°íšŒ (created_time ì œì™¸)
        ìºì‹±í•˜ì—¬ ë°˜ë³µ ì¡°íšŒ ë°©ì§€
        """
        # ìºì‹œ í™•ì¸
        if table_name in self.table_column_count_cache:
            return self.table_column_count_cache[table_name]
        
        # Dry-run ëª¨ë“œì—ì„œëŠ” channel_routerì—ì„œ ì˜ˆìƒ ì»¬ëŸ¼ ìˆ˜ ê°€ì ¸ì˜¤ê¸°
        if self.dry_run:
            # í…Œì´ë¸” íƒ€ì… ì¶”ì¶œ (tbl_data_timeseries_imo9976903_1 -> '1')
            table_type = table_name.split('_')[-1]
            if table_type in ['1', '2', '3']:
                col_count = len(self.channel_router.get_all_channels_by_table(table_type))
                self.table_column_count_cache[table_name] = col_count
                return col_count
            return 0
        
        try:
            query = """
                SELECT COUNT(*) as col_count
                FROM information_schema.columns
                WHERE table_schema = 'tenant'
                  AND table_name = %s
                  AND column_name != 'created_time'
            """
            
            result = db_manager.execute_query(query, (table_name,))
            if result:
                col_count = result[0]['col_count']
                # ìºì‹œ ì €ì¥
                self.table_column_count_cache[table_name] = col_count
                return col_count
            else:
                return 0
                
        except Exception as e:
            logger.warning(f"   âš ï¸ Could not get column count for {table_name}: {e}")
            return 0
    
    def check_table_columns(self, imo_number: str):
        """í…Œì´ë¸”ì˜ ì‹¤ì œ ì»¬ëŸ¼ ê°œìˆ˜ í™•ì¸ ë° ê²½ê³ """
        try:
            for table_type in ['1', '2', '3']:
                table_name = f"tbl_data_timeseries_{imo_number.lower()}_{table_type}"
                col_count = self.get_table_column_count(table_name)
                logger.info(f"   ğŸ“Š Table {table_type}: {col_count} data columns")
                    
        except Exception as e:
            logger.warning(f"   âš ï¸ Could not check table columns: {e}")
    
    def upsert_batch_data(self, imo_number: str, batch_data: Dict[str, List[Dict]], 
                          channels_by_table: Dict[str, List[str]]):
        """ë°°ì¹˜ ë°ì´í„°ë¥¼ ê° í…Œì´ë¸”ì— upsert"""
        for table_type, rows in batch_data.items():
            if not rows:
                continue
            
            table_name = f"tbl_data_timeseries_{imo_number.lower()}_{table_type}"
            channel_list = channels_by_table[table_type]
            
            self.upsert_to_table(table_name, rows, channel_list, table_type)
    
    def upsert_to_table(self, table_name: str, rows: List[Dict], channel_list: List[str], table_type: str):
        """
        íŠ¹ì • í…Œì´ë¸”ì— ë°ì´í„° upsert
        
        Args:
            table_name: í…Œì´ë¸”ëª…
            rows: upsertí•  row ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            channel_list: ì±„ë„ ID ë¦¬ìŠ¤íŠ¸
            table_type: í…Œì´ë¸” íƒ€ì… ('1', '2', '3')
        """
        if not rows:
            return
        
        # Dry-run ëª¨ë“œ
        if self.dry_run:
            # í…Œì´ë¸”ë³„ í†µê³„ ì—…ë°ì´íŠ¸
            stat_key = f'table_{table_type}_rows'
            self.stats[stat_key] += len(rows)
            logger.debug(f"         ğŸ” [DRY-RUN] Would upsert {len(rows)} rows to {table_name}")
            return
        
        # SQL ì¿¼ë¦¬ ìƒì„±
        # ì»¬ëŸ¼ëª… quoting (íŠ¹ìˆ˜ë¬¸ì í¬í•¨)
        quoted_columns = [f'"{col}"' for col in channel_list]
        all_columns = ['created_time'] + quoted_columns
        
        # INSERT êµ¬ë¬¸
        columns_str = ', '.join(all_columns)
        placeholders = ', '.join(['%s'] * len(all_columns))
        
        # UPDATE êµ¬ë¬¸ (created_time ì œì™¸)
        # NULLì´ ì•„ë‹Œ ê°’ë§Œ UPDATE (ë¹ˆ ê°’ì€ ê¸°ì¡´ ê°’ ìœ ì§€)
        update_set_parts = []
        for col in channel_list:
            # CASE WHENì„ ì‚¬ìš©í•´ì„œ NULLì´ ì•„ë‹ ë•Œë§Œ ì—…ë°ì´íŠ¸
            update_set_parts.append(
                f'"{col}" = CASE WHEN EXCLUDED."{col}" IS NOT NULL THEN EXCLUDED."{col}" ELSE {table_name}."{col}" END'
            )
        update_set = ', '.join(update_set_parts)
        
        upsert_query = f"""
            INSERT INTO tenant.{table_name} ({columns_str})
            VALUES ({placeholders})
            ON CONFLICT (created_time) 
            DO UPDATE SET {update_set}
        """
        
        # ë°ì´í„° ì¤€ë¹„
        values_list = []
        for row in rows:
            values = [row['created_time']]
            for channel_id in channel_list:
                values.append(row.get(channel_id))
            values_list.append(tuple(values))
        
        # ì‹¤í–‰
        try:
            conn = db_manager.get_connection()
            cursor = conn.cursor()
            
            # executemanyë¡œ ë°°ì¹˜ ì²˜ë¦¬
            from psycopg2.extras import execute_batch
            execute_batch(cursor, upsert_query, values_list, page_size=1000)
            
            conn.commit()
            cursor.close()
            db_manager.return_connection(conn)
            
            # í…Œì´ë¸”ë³„ í†µê³„ ì—…ë°ì´íŠ¸
            stat_key = f'table_{table_type}_rows'
            self.stats[stat_key] += len(rows)
            
            # Coverage ì •ë³´ì™€ í•¨ê»˜ ë¡œê¹…
            channel_count = len(channel_list)
            logger.info(f"         âœ… Upserted {len(rows)} rows to {table_name}")
            logger.info(f"            Affected columns: {channel_count} (other columns: NULL for INSERT, unchanged for UPDATE)")
            
        except Exception as e:
            logger.error(f"         âŒ Upsert failed for {table_name}: {e}")
            logger.error(f"         Sample row: {rows[0] if rows else 'N/A'}")
            if 'conn' in locals():
                conn.rollback()
                db_manager.return_connection(conn)
            raise
    
    def print_summary(self):
        """ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        logger.info(f"\n{'='*80}")
        logger.info("ğŸ“Š UPSERT SUMMARY")
        logger.info(f"{'='*80}")
        logger.info(f"ğŸ“ Files:")
        logger.info(f"   Total found: {self.stats['total_files']}")
        logger.info(f"   âœ… Processed: {self.stats['processed_files']}")
        logger.info(f"   âŒ Failed: {self.stats['failed_files']}")
        logger.info(f"")
        logger.info(f"ğŸ“Š CSV Rows:")
        logger.info(f"   Total read from CSV: {self.stats['csv_rows_read']:,}")
        logger.info(f"")
        logger.info(f"ğŸ’¾ DB Rows Upserted (per table):")
        logger.info(f"   Table 1 (Auxiliary): {self.stats['table_1_rows']:,}")
        logger.info(f"   Table 2 (Engine/Generator): {self.stats['table_2_rows']:,}")
        logger.info(f"   Table 3 (Navigation/Ship): {self.stats['table_3_rows']:,}")
        total_db_rows = self.stats['table_1_rows'] + self.stats['table_2_rows'] + self.stats['table_3_rows']
        logger.info(f"   Total DB rows: {total_db_rows:,}")
        logger.info(f"")
        logger.info(f"â„¹ï¸  Note:")
        logger.info(f"   - 1 CSV row â†’ up to 3 DB rows (one per table)")
        logger.info(f"   - For EXISTING rows: CSV columns updated, others unchanged")
        logger.info(f"   - For NEW rows: CSV columns filled, others set to NULL")
        logger.info(f"{'='*80}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Upsert CSV migration data to wide tables')
    parser.add_argument(
        '--dir', 
        type=str, 
        default='migration_data',
        help='Base directory containing ship folders (default: migration_data)'
    )
    parser.add_argument(
        '--ship',
        type=str,
        help='Process only specific ship code (e.g., H2546)'
    )
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='Dry-run mode: check files and channels without inserting data'
    )
    
    args = parser.parse_args()
    
    try:
        upserter = CSVMigrationUpserter(base_dir=args.dir, dry_run=args.dry_run)
        
        if args.ship:
            # íŠ¹ì • ì„ ë°•ë§Œ ì²˜ë¦¬
            ship_folder = Path(args.dir) / args.ship
            if not ship_folder.exists():
                logger.error(f"âŒ Ship folder not found: {ship_folder}")
                return 1
            
            if args.ship not in SHIP_MAPPING:
                logger.error(f"âŒ Unknown ship code: {args.ship}")
                return 1
            
            imo_number = SHIP_MAPPING[args.ship]
            logger.info(f"ğŸš¢ Processing single ship: {args.ship} â†’ {imo_number}")
            upserter.process_ship_folder(ship_folder, imo_number)
        else:
            # ëª¨ë“  ì„ ë°• ì²˜ë¦¬
            upserter.process_all_ships()
        
        return 0
        
    except KeyboardInterrupt:
        logger.warning("\nâš ï¸ Process interrupted by user")
        return 1
    except Exception as e:
        logger.error(f"\nâŒ Fatal error: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return 1


if __name__ == "__main__":
    exit(main())

