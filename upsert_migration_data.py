"""
CSV Migration Data Upserter
migration_data í´ë”ì˜ CSV íŒŒì¼ë“¤ì„ ì½ì–´ì„œ 3ê°œì˜ wide í…Œì´ë¸”ì— upsert
"""
import os
import csv
from datetime import datetime
from typing import Dict, List, Optional
from pathlib import Path
from loguru import logger
import sys

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ import
from database import db_manager
from channel_router import channel_router
from config import migration_config

# ì„ ë°• ë²ˆí˜¸ ë§¤í•‘
SHIP_MAPPING = {
    'H2546': 'IMO9976903',
    'H2547': 'IMO9976915',
    'H2548': 'IMO9976927',
    'H2549': 'IMO9976939',
    'H2559': 'IMO9986051',
    'H2560': 'IMO9986087',
}

# ë¡œê¹… ì„¤ì •
logger.remove()
logger.add(
    "logs/csv_upsert.log",
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}",
    level="INFO",
    rotation="100 MB"
)
logger.add(
    sys.stdout,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level}</level> | {message}",
    level="INFO"
)


class CSVMigrationUpserter:
    """CSV íŒŒì¼ì„ ì½ì–´ì„œ wide í…Œì´ë¸”ì— upsertí•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, base_dir: str = "migration_data", dry_run: bool = False):
        self.base_dir = Path(base_dir)
        self.channel_router = channel_router
        self.dry_run = dry_run
        self.stats = {
            'total_files': 0,
            'processed_files': 0,
            'failed_files': 0,
            'total_rows': 0,
            'inserted_rows': 0,
            'updated_rows': 0
        }
        
        if dry_run:
            logger.warning("ğŸ” DRY-RUN MODE: No data will be inserted into DB")
    
    def process_all_ships(self):
        """ëª¨ë“  ì„ ë°•ì˜ CSV íŒŒì¼ ì²˜ë¦¬"""
        logger.info("ğŸš€ Starting CSV migration data upsert")
        logger.info(f"ğŸ“‚ Base directory: {self.base_dir.absolute()}")
        
        if not self.base_dir.exists():
            logger.error(f"âŒ Directory not found: {self.base_dir}")
            return
        
        # ê° ì„ ë°• í´ë” ì²˜ë¦¬
        for ship_folder in sorted(self.base_dir.iterdir()):
            if not ship_folder.is_dir():
                continue
            
            ship_code = ship_folder.name
            if ship_code not in SHIP_MAPPING:
                logger.warning(f"âš ï¸ Unknown ship code: {ship_code}, skipping...")
                continue
            
            imo_number = SHIP_MAPPING[ship_code]
            logger.info(f"\n{'='*80}")
            logger.info(f"ğŸš¢ Processing ship: {ship_code} â†’ {imo_number}")
            logger.info(f"{'='*80}")
            
            self.process_ship_folder(ship_folder, imo_number)
        
        # ìµœì¢… í†µê³„
        self.print_summary()
    
    def process_ship_folder(self, ship_folder: Path, imo_number: str):
        """íŠ¹ì • ì„ ë°• í´ë”ì˜ ëª¨ë“  CSV íŒŒì¼ ì²˜ë¦¬"""
        csv_files = sorted(ship_folder.glob("*.csv"))
        
        if not csv_files:
            logger.warning(f"   âš ï¸ No CSV files found in {ship_folder}")
            return
        
        logger.info(f"   ğŸ“Š Found {len(csv_files)} CSV files")
        
        # í…Œì´ë¸” ì¡´ì¬ í™•ì¸ (Dry-runì´ ì•„ë‹ ë•Œë§Œ)
        if not self.dry_run:
            logger.info(f"   ğŸ” Checking if tables exist for {imo_number}...")
            for table_type in ['1', '2', '3']:
                table_name = f"tbl_data_timeseries_{imo_number.lower()}_{table_type}"
                if not db_manager.check_table_exists(table_name):
                    logger.error(f"   âŒ Table does not exist: {table_name}")
                    logger.error(f"   ğŸ’¡ Run Realtime or Batch first to create tables, or use multi_table_generator")
                    raise RuntimeError(f"Table {table_name} does not exist")
            logger.info(f"   âœ… All 3 tables exist")
        
        for csv_file in csv_files:
            try:
                self.process_csv_file(csv_file, imo_number)
                self.stats['processed_files'] += 1
            except Exception as e:
                logger.error(f"   âŒ Failed to process {csv_file.name}: {e}")
                self.stats['failed_files'] += 1
    
    def process_csv_file(self, csv_file: Path, imo_number: str):
        """ë‹¨ì¼ CSV íŒŒì¼ ì²˜ë¦¬"""
        logger.info(f"\n   ğŸ“„ Processing: {csv_file.name}")
        self.stats['total_files'] += 1
        
        # CSV ì½ê¸°
        with open(csv_file, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            
            # í—¤ë”ì—ì„œ ì±„ë„ ëª©ë¡ ì¶”ì¶œ
            fieldnames = reader.fieldnames
            if not fieldnames or 'timestamp' not in fieldnames:
                raise ValueError(f"Invalid CSV format: missing 'timestamp' column")
            
            channel_ids = [col for col in fieldnames if col != 'timestamp']
            logger.info(f"      ğŸ“Š Columns: {len(channel_ids)} channels")
            
            # ì±„ë„ì„ í…Œì´ë¸”ë³„ë¡œ ë¶„ë¥˜
            channels_by_table = self.classify_channels(channel_ids)
            
            # ë§¤ì¹­ëœ ì±„ë„ ì´ ìˆ˜ í™•ì¸
            total_matched = sum(len(chs) for chs in channels_by_table.values())
            if total_matched == 0:
                logger.error(f"      âŒ No channels matched! All {len(channel_ids)} channels are unknown.")
                logger.error(f"         Sample unmapped channels: {channel_ids[:5]}")
                raise ValueError(f"No channels matched for {csv_file.name}")
            
            if total_matched < len(channel_ids):
                unmapped_count = len(channel_ids) - total_matched
                logger.warning(f"      âš ï¸ {unmapped_count}/{len(channel_ids)} channels not mapped (will be skipped)")
            
            # í…Œì´ë¸”ë³„ í†µê³„
            for table_type, channels in channels_by_table.items():
                logger.info(f"      - Table {table_type}: {len(channels)} channels")
            
            # ë°ì´í„° ì²˜ë¦¬
            rows_processed = 0
            batch_size = 1000
            batch_data = {
                '1': [],
                '2': [],
                '3': []
            }
            
            for row in reader:
                timestamp_str = row['timestamp']
                
                # timestamp íŒŒì‹±
                try:
                    timestamp = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S')
                except ValueError:
                    logger.warning(f"      âš ï¸ Invalid timestamp: {timestamp_str}, skipping row")
                    continue
                
                # í…Œì´ë¸”ë³„ë¡œ ë°ì´í„° ì¤€ë¹„
                for table_type, table_channels in channels_by_table.items():
                    # ì´ í…Œì´ë¸”ì— ë§¤ì¹­ë˜ëŠ” ì±„ë„ì´ ì—†ìœ¼ë©´ skip
                    if not table_channels:
                        continue
                    
                    row_data = {'created_time': timestamp}
                    has_valid_data = False
                    
                    for channel_id in table_channels:
                        value_str = row.get(channel_id, '')
                        if value_str and value_str.strip():
                            try:
                                value = float(value_str)
                                row_data[channel_id] = value
                                has_valid_data = True  # ìœ íš¨í•œ ë°ì´í„°ê°€ í•˜ë‚˜ë¼ë„ ìˆìŒ
                            except ValueError:
                                # ë³€í™˜ ì‹¤íŒ¨ ì‹œ None
                                row_data[channel_id] = None
                        else:
                            row_data[channel_id] = None
                    
                    # ìœ íš¨í•œ ë°ì´í„°ê°€ í•˜ë‚˜ë¼ë„ ìˆì„ ë•Œë§Œ ì¶”ê°€
                    # (created_timeë§Œ ìˆëŠ” ë¹ˆ row ë°©ì§€)
                    if has_valid_data:
                        batch_data[table_type].append(row_data)
                
                rows_processed += 1
                
                # ë°°ì¹˜ ì²˜ë¦¬
                if rows_processed % batch_size == 0:
                    self.upsert_batch_data(imo_number, batch_data, channels_by_table)
                    batch_data = {'1': [], '2': [], '3': []}
                    logger.info(f"      â³ Processed {rows_processed} rows...")
            
            # ë‚¨ì€ ë°ì´í„° ì²˜ë¦¬
            if any(batch_data.values()):
                self.upsert_batch_data(imo_number, batch_data, channels_by_table)
            
            self.stats['total_rows'] += rows_processed
            logger.success(f"      âœ… Completed: {rows_processed} rows processed")
    
    def classify_channels(self, channel_ids: List[str]) -> Dict[str, List[str]]:
        """ì±„ë„ì„ í…Œì´ë¸”ë³„ë¡œ ë¶„ë¥˜"""
        channels_by_table = {
            '1': [],  # auxiliary_systems
            '2': [],  # engine_generator
            '3': []   # navigation_ship
        }
        
        for channel_id in channel_ids:
            table_type = self.channel_router.get_table_type(channel_id)
            if table_type:
                channels_by_table[table_type].append(channel_id)
            else:
                logger.debug(f"         âš ï¸ Channel not mapped: {channel_id}")
        
        return channels_by_table
    
    def upsert_batch_data(self, imo_number: str, batch_data: Dict[str, List[Dict]], 
                          channels_by_table: Dict[str, List[str]]):
        """ë°°ì¹˜ ë°ì´í„°ë¥¼ ê° í…Œì´ë¸”ì— upsert"""
        for table_type, rows in batch_data.items():
            if not rows:
                continue
            
            table_name = f"tbl_data_timeseries_{imo_number.lower()}_{table_type}"
            channel_list = channels_by_table[table_type]
            
            self.upsert_to_table(table_name, rows, channel_list)
    
    def upsert_to_table(self, table_name: str, rows: List[Dict], channel_list: List[str]):
        """íŠ¹ì • í…Œì´ë¸”ì— ë°ì´í„° upsert"""
        if not rows:
            return
        
        # Dry-run ëª¨ë“œ
        if self.dry_run:
            self.stats['inserted_rows'] += len(rows)
            logger.debug(f"         ğŸ” [DRY-RUN] Would upsert {len(rows)} rows to {table_name}")
            return
        
        # SQL ì¿¼ë¦¬ ìƒì„±
        # ì»¬ëŸ¼ëª… quoting (íŠ¹ìˆ˜ë¬¸ì í¬í•¨)
        quoted_columns = [f'"{col}"' for col in channel_list]
        all_columns = ['created_time'] + quoted_columns
        
        # INSERT êµ¬ë¬¸
        columns_str = ', '.join(all_columns)
        placeholders = ', '.join(['%s'] * len(all_columns))
        
        # UPDATE êµ¬ë¬¸ (created_time ì œì™¸)
        update_set = ', '.join([f'"{col}" = EXCLUDED."{col}"' for col in channel_list])
        
        upsert_query = f"""
            INSERT INTO tenant.{table_name} ({columns_str})
            VALUES ({placeholders})
            ON CONFLICT (created_time) 
            DO UPDATE SET {update_set}
        """
        
        # ë°ì´í„° ì¤€ë¹„
        values_list = []
        for row in rows:
            values = [row['created_time']]
            for channel_id in channel_list:
                values.append(row.get(channel_id))
            values_list.append(tuple(values))
        
        # ì‹¤í–‰
        try:
            conn = db_manager.get_connection()
            cursor = conn.cursor()
            
            # executemanyë¡œ ë°°ì¹˜ ì²˜ë¦¬
            from psycopg2.extras import execute_batch
            execute_batch(cursor, upsert_query, values_list, page_size=1000)
            
            conn.commit()
            cursor.close()
            db_manager.return_connection(conn)
            
            self.stats['inserted_rows'] += len(rows)
            logger.debug(f"         âœ… Upserted {len(rows)} rows to {table_name}")
            
        except Exception as e:
            logger.error(f"         âŒ Upsert failed for {table_name}: {e}")
            logger.error(f"         Sample row: {rows[0] if rows else 'N/A'}")
            if 'conn' in locals():
                conn.rollback()
                db_manager.return_connection(conn)
            raise
    
    def print_summary(self):
        """ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        logger.info(f"\n{'='*80}")
        logger.info("ğŸ“Š UPSERT SUMMARY")
        logger.info(f"{'='*80}")
        logger.info(f"ğŸ“ Total files found: {self.stats['total_files']}")
        logger.info(f"âœ… Successfully processed: {self.stats['processed_files']}")
        logger.info(f"âŒ Failed: {self.stats['failed_files']}")
        logger.info(f"ğŸ“Š Total rows processed: {self.stats['total_rows']:,}")
        logger.info(f"ğŸ’¾ Total rows upserted: {self.stats['inserted_rows']:,}")
        logger.info(f"{'='*80}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Upsert CSV migration data to wide tables')
    parser.add_argument(
        '--dir', 
        type=str, 
        default='migration_data',
        help='Base directory containing ship folders (default: migration_data)'
    )
    parser.add_argument(
        '--ship',
        type=str,
        help='Process only specific ship code (e.g., H2546)'
    )
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='Dry-run mode: check files and channels without inserting data'
    )
    
    args = parser.parse_args()
    
    try:
        upserter = CSVMigrationUpserter(base_dir=args.dir, dry_run=args.dry_run)
        
        if args.ship:
            # íŠ¹ì • ì„ ë°•ë§Œ ì²˜ë¦¬
            ship_folder = Path(args.dir) / args.ship
            if not ship_folder.exists():
                logger.error(f"âŒ Ship folder not found: {ship_folder}")
                return 1
            
            if args.ship not in SHIP_MAPPING:
                logger.error(f"âŒ Unknown ship code: {args.ship}")
                return 1
            
            imo_number = SHIP_MAPPING[args.ship]
            logger.info(f"ğŸš¢ Processing single ship: {args.ship} â†’ {imo_number}")
            upserter.process_ship_folder(ship_folder, imo_number)
        else:
            # ëª¨ë“  ì„ ë°• ì²˜ë¦¬
            upserter.process_all_ships()
        
        return 0
        
    except KeyboardInterrupt:
        logger.warning("\nâš ï¸ Process interrupted by user")
        return 1
    except Exception as e:
        logger.error(f"\nâŒ Fatal error: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return 1


if __name__ == "__main__":
    exit(main())

